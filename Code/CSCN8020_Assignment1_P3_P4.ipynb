{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c50c9ba",
      "metadata": {},
      "source": [
        "# CSCN8020 — Assignment 1\n",
        "## Problems 3 & 4: Value Iteration and Off-Policy Monte Carlo\n",
        "\n",
        "**Instructions**: Run all cells. This notebook will compute V* and π* for the 5×5 gridworld\n",
        "(standard & in-place VI), and then estimate V using off-policy Monte Carlo with importance sampling.\n",
        "CSV files will be written to the working directory for inclusion in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "44e79f5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: V_star_standard.csv, V_star_inplace.csv, policy_star_standard.csv, policy_star_inplace.csv, V_mc_offpolicy.csv, policy_mc_offpolicy.csv\n"
          ]
        }
      ],
      "source": [
        "# CSCN8020 Assignment 1 — Problems 3 & 4\n",
        "# Value Iteration (standard & in-place) for 5x5 Gridworld + Off-policy Monte Carlo with Importance Sampling\n",
        "# Run this script to regenerate the CSV outputs used in the report.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Tuple, Dict, List, Optional\n",
        "import random\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Gridworld5x5:\n",
        "    nrows: int = 5\n",
        "    ncols: int = 5\n",
        "    gamma: float = 0.9\n",
        "    goal: Tuple[int, int] = (4, 4)\n",
        "    greys: Tuple[Tuple[int, int], ...] = ((2, 2), (3, 0), (0, 4))  # s2,2, s3,0, s0,4\n",
        "    # Actions: right, down, left, up\n",
        "    actions: Tuple[Tuple[int, int], ...] = ((0, 1), (1, 0), (0, -1), (-1, 0))\n",
        "\n",
        "    def in_bounds(self, r: int, c: int) -> bool:\n",
        "        return 0 <= r < self.nrows and 0 <= c < self.ncols\n",
        "\n",
        "    def step(self, state: Tuple[int, int], action_idx: int) -> Tuple[Tuple[int, int], float, bool]:\n",
        "        r, c = state\n",
        "        dr, dc = self.actions[action_idx]\n",
        "        nr, nc = r + dr, c + dc\n",
        "        if not self.in_bounds(nr, nc):\n",
        "            nr, nc = r, c  # invalid action => stay\n",
        "        next_state = (nr, nc)\n",
        "\n",
        "        # Rewards\n",
        "        if next_state == self.goal:\n",
        "            reward = 10.0\n",
        "        elif next_state in self.greys:\n",
        "            reward = -5.0\n",
        "        else:\n",
        "            reward = -1.0\n",
        "\n",
        "        done = (next_state == self.goal)\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def reward_of(self, state: Tuple[int, int]) -> float:\n",
        "        if state == self.goal:\n",
        "            return 10.0\n",
        "        if state in self.greys:\n",
        "            return -5.0\n",
        "        return -1.0\n",
        "\n",
        "    def states(self) -> List[Tuple[int, int]]:\n",
        "        return [(r, c) for r in range(self.nrows) for c in range(self.ncols)]\n",
        "\n",
        "\n",
        "def idx_map(env: Gridworld5x5) -> Dict[Tuple[int, int], int]:\n",
        "    return {s: i for i, s in enumerate(env.states())}\n",
        "\n",
        "\n",
        "def greedy_policy_from_V(env: Gridworld5x5, V: np.ndarray) -> np.ndarray:\n",
        "    s2i = idx_map(env)\n",
        "    A = len(env.actions)\n",
        "    policy = np.zeros(len(s2i), dtype=int)\n",
        "    for s, i in s2i.items():\n",
        "        if s == env.goal:\n",
        "            policy[i] = 0\n",
        "            continue\n",
        "        q_values = []\n",
        "        for a in range(A):\n",
        "            ns, r, _ = env.step(s, a)\n",
        "            q_values.append(r + env.gamma * V[s2i[ns]])\n",
        "        policy[i] = int(np.argmax(q_values))\n",
        "    return policy\n",
        "\n",
        "\n",
        "def arrows_for_actions(actions):\n",
        "    arrow_map = {0: \"→\", 1: \"↓\", 2: \"←\", 3: \"↑\"}\n",
        "    return [arrow_map[int(a)] for a in actions]\n",
        "\n",
        "\n",
        "def value_iteration(env: Gridworld5x5, theta: float = 1e-6, in_place: bool = False):\n",
        "    s2i = idx_map(env)\n",
        "    N = len(s2i)\n",
        "    A = len(env.actions)\n",
        "    V = np.zeros(N, dtype=float)\n",
        "    iters = 0\n",
        "\n",
        "    while True:\n",
        "        delta = 0.0\n",
        "        if in_place:\n",
        "            for s, i in s2i.items():\n",
        "                if s == env.goal:\n",
        "                    continue\n",
        "                q_values = []\n",
        "                for a in range(A):\n",
        "                    ns, r, _ = env.step(s, a)\n",
        "                    q_values.append(r + env.gamma * V[s2i[ns]])\n",
        "                new_v = max(q_values)\n",
        "                delta = max(delta, abs(new_v - V[i]))\n",
        "                V[i] = new_v\n",
        "        else:\n",
        "            V_new = V.copy()\n",
        "            for s, i in s2i.items():\n",
        "                if s == env.goal:\n",
        "                    continue\n",
        "                q_values = []\n",
        "                for a in range(A):\n",
        "                    ns, r, _ = env.step(s, a)\n",
        "                    q_values.append(r + env.gamma * V[s2i[ns]])\n",
        "                V_new[i] = max(q_values)\n",
        "                delta = max(delta, abs(V_new[i] - V[i]))\n",
        "            V = V_new\n",
        "\n",
        "        iters += 1\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    policy = greedy_policy_from_V(env, V)\n",
        "    return V, policy, iters\n",
        "\n",
        "\n",
        "def generate_episode(env: Gridworld5x5, start=None, behavior_probs=None, max_steps: int = 500):\n",
        "    s2i = idx_map(env)\n",
        "    A = len(env.actions)\n",
        "    if behavior_probs is None:\n",
        "        behavior_probs = [1.0 / A] * A\n",
        "\n",
        "    if start is None:\n",
        "        candidates = [s for s in env.states() if s != env.goal]\n",
        "        state = random.choice(candidates)\n",
        "    else:\n",
        "        state = start\n",
        "\n",
        "    trajectory = []\n",
        "    for t in range(max_steps):\n",
        "        a = np.random.choice(np.arange(A), p=np.array(behavior_probs))\n",
        "        next_state, reward, done = env.step(state, a)\n",
        "        trajectory.append((state, a, reward))\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "    return trajectory\n",
        "\n",
        "\n",
        "def off_policy_mc_importance_sampling(env: Gridworld5x5, episodes: int = 10000, gamma: float = 0.9, behavior_probs=None):\n",
        "    s2i = idx_map(env)\n",
        "    A = len(env.actions)\n",
        "    if behavior_probs is None:\n",
        "        behavior_probs = [1.0 / A] * A  # uniform random\n",
        "\n",
        "    V_est = np.zeros(len(s2i), dtype=float)\n",
        "    C = np.zeros(len(s2i), dtype=float)  # cumulative weights\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        target_pi = greedy_policy_from_V(env, V_est)\n",
        "        traj = generate_episode(env, behavior_probs=behavior_probs, max_steps=200)\n",
        "        G = 0.0\n",
        "        W = 1.0\n",
        "\n",
        "        for t in reversed(range(len(traj))):\n",
        "            s, a, r = traj[t]\n",
        "            G = gamma * G + r\n",
        "            idx = s2i[s]\n",
        "            if a != target_pi[idx]:\n",
        "                W = 0.0\n",
        "            else:\n",
        "                W = W * (1.0 / behavior_probs[a])\n",
        "            if W == 0.0:\n",
        "                break\n",
        "            C[idx] += W\n",
        "            V_est[idx] += (W / C[idx]) * (G - V_est[idx])\n",
        "\n",
        "    return V_est, target_pi\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = Gridworld5x5()\n",
        "\n",
        "    # Problem 3\n",
        "    V_std, pi_std, it_std = value_iteration(env, theta=1e-8, in_place=False)\n",
        "    V_inp, pi_inp, it_inp = value_iteration(env, theta=1e-8, in_place=True)\n",
        "\n",
        "    # Save grids\n",
        "    s2i = idx_map(env)\n",
        "    V_std_grid = np.array([V_std[s2i[(r, c)]] for r in range(env.nrows) for c in range(env.ncols)]).reshape(env.nrows, env.ncols)\n",
        "    V_inp_grid = np.array([V_inp[s2i[(r, c)]] for r in range(env.nrows) for c in range(env.ncols)]).reshape(env.nrows, env.ncols)\n",
        "\n",
        "    pi_std_arrows = np.array(arrows_for_actions(list(pi_std))).reshape(env.nrows, env.ncols)\n",
        "    pi_inp_arrows = np.array(arrows_for_actions(list(pi_inp))).reshape(env.nrows, env.ncols)\n",
        "\n",
        "    pd.DataFrame(V_std_grid).to_csv(\"V_star_standard.csv\", index=False)\n",
        "    pd.DataFrame(V_inp_grid).to_csv(\"V_star_inplace.csv\", index=False)\n",
        "    pd.DataFrame(pi_std_arrows).to_csv(\"policy_star_standard.csv\", index=False)\n",
        "    pd.DataFrame(pi_inp_arrows).to_csv(\"policy_star_inplace.csv\", index=False)\n",
        "\n",
        "    # Problem 4\n",
        "    V_mc, pi_mc = off_policy_mc_importance_sampling(env, episodes=8000, gamma=env.gamma)\n",
        "    V_mc_grid = np.array([V_mc[s2i[(r, c)]] for r in range(env.nrows) for c in range(env.ncols)]).reshape(env.nrows, env.ncols)\n",
        "    pi_mc_arrows = np.array(arrows_for_actions(list(pi_mc))).reshape(env.nrows, env.ncols)\n",
        "\n",
        "    pd.DataFrame(V_mc_grid).to_csv(\"V_mc_offpolicy.csv\", index=False)\n",
        "    pd.DataFrame(pi_mc_arrows).to_csv(\"policy_mc_offpolicy.csv\", index=False)\n",
        "\n",
        "    print(\"Saved: V_star_standard.csv, V_star_inplace.csv, policy_star_standard.csv, policy_star_inplace.csv, V_mc_offpolicy.csv, policy_mc_offpolicy.csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e962934f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: V_star_standard.csv, V_star_inplace.csv, policy_star_standard.csv, policy_star_inplace.csv, V_mc_offpolicy.csv, policy_mc_offpolicy.csv\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a694b79",
      "metadata": {},
      "source": [
        "### Files produced\n",
        "- `V_star_standard.csv`\n",
        "- `V_star_inplace.csv`\n",
        "- `policy_star_standard.csv`\n",
        "- `policy_star_inplace.csv`\n",
        "- `V_mc_offpolicy.csv`\n",
        "- `policy_mc_offpolicy.csv`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
